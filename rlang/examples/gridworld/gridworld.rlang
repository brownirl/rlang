# To start, we imported a vocabulary file contains metadata on the MDP
# and references to additional RLang groundings.

import "vocab.json"

# The position of the agent as well as its x and y coordinates are defined
# as Factors, each representing a portion of the state space.

Factor position := S[0, 1]
Factor x := position[0]
Factor y := position[1]

Constant lava_locs := [[3, 2], [1, 4], [2, 4], [2, 5]]

Proposition reached_goal := x == 5 and y == 1 
Proposition reached_wall := x == 3 and y == 1
Proposition in_lava := position in lava_locs

# The following Actions correspond to four 
# discrete actions the agent can take:
#   0 - move up
#   1 - move down
#   2 - move left
#   3 - move right

Action up := 0
Action down := 1
Action left := 2
Action right := 3

# The predicted consequence of each of these actions is 
# specified using an Effect: the position of the agent in the next state 
# and the next state itself will update accordingly.

Effect action_effect:
    if A == up:
        position' -> position + [0, 1]
        S' -> S + [0, 1]
    elif A == down:
        position' -> position + [0, -1]
        S' -> S + [0, -1]
    elif A == left:
        position' -> position + [-1, 0]
        S' -> S + [-1, 0]
    elif A == right:
        position' -> position + [1, 0]
        S' -> S + [1, 0]


# A reward of 1 is given for reaching the goal coordinates,
# and -1 for moving on lava. 
# The following Effect references the previously defined Propositions.

Effect main:
    if in_lava:
        Reward -1
    if reached_goal:
        Reward 1
    if reached_wall:
        S' -> S     # state remains the same.
    else:
        -> action_effect
